{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InformationRetrieve.py",
      "provenance": [],
      "authorship_tag": "ABX9TyO890mmy/Vn62h60YexhJyF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce2_lWCh_ixc",
        "outputId": "4625f2ff-9a96-498c-84cc-88e973f9f60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.7/dist-packages (1.26.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Question:  How to increase mortgage amount ? Do I need more loan sections?\n",
            "to increase your money loan obvion adds one or more new loan sections\n",
            "the total money loan may not exceed the amount for which the mortgage was granted\n",
            "you must arrange to increase your money loan through an intermediary\n",
            "then you may increase your loan to that amount without having to \n",
            " \n",
            "then you can increase your loan up to that amount without having to go to the notary\n",
            "obvion is however not obliged to agree to a request for an increase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "RuntimeWarning: invalid value encountered in true_divide [ipykernel_launcher.py:83]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim import corpora\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "import heapq\n",
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfFileReader\n",
        "\n",
        "\n",
        "\n",
        "file = open('Article 26 Increase of the loan.pdf','rb')\n",
        "PDF_read = PdfFileReader(file)\n",
        "txt=\"\"\n",
        "for i in range(PDF_read.numPages):\n",
        "    page_temp = PDF_read.getPage(i)\n",
        "    page_String= page_temp.extractText()\n",
        "    txt=txt+ page_String\n",
        "#class for preprocessing and creating word embedding\n",
        "class Preprocessing:\n",
        "    #constructor\n",
        "    def __init__(self,txt):\n",
        "        # Tokenization\n",
        "        nltk.download('punkt')  #punkt is nltk tokenizer \n",
        "        # breaking text to sentences\n",
        "        tokens = nltk.sent_tokenize(txt) \n",
        "        self.tokens = tokens\n",
        "        self.tfidfvectoriser=TfidfVectorizer()\n",
        "\n",
        "    # Data Cleaning\n",
        "    # remove extra spaces\n",
        "    # convert sentences to lower case \n",
        "    # remove stopword\n",
        "    def clean_sentence(self, sentence, stopwords=False):\n",
        "        sentence = sentence.lower().strip()\n",
        "        sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
        "        if stopwords:\n",
        "            sentence = remove_stopwords(sentence)\n",
        "        return sentence\n",
        "\n",
        "    # store cleaned sentences to cleaned_sentences\n",
        "    def get_cleaned_sentences(self,tokens, stopwords=False):\n",
        "        cleaned_sentences = []\n",
        "        for line in tokens:\n",
        "            cleaned = self.clean_sentence(line, stopwords)\n",
        "            cleaned_sentences.append(cleaned)\n",
        "        return cleaned_sentences\n",
        "\n",
        "    #do all the cleaning\n",
        "    def cleanall(self):\n",
        "        cleaned_sentences = self.get_cleaned_sentences(self.tokens, stopwords=True)\n",
        "        cleaned_sentences_with_stopwords = self.get_cleaned_sentences(self.tokens, stopwords=False)\n",
        "        # print(cleaned_sentences)\n",
        "        # print(cleaned_sentences_with_stopwords)\n",
        "        return [cleaned_sentences,cleaned_sentences_with_stopwords]\n",
        "\n",
        "    # TF-IDF Vectorizer\n",
        "    def TFIDF(self,cleaned_sentences):\n",
        "        self.tfidfvectoriser.fit(cleaned_sentences)\n",
        "        tfidf_vectors=self.tfidfvectoriser.transform(cleaned_sentences)\n",
        "        return tfidf_vectors\n",
        "\n",
        "    #tfidf for question\n",
        "    def TFIDF_Q(self,question_to_be_cleaned):\n",
        "        tfidf_vectors=self.tfidfvectoriser.transform([question_to_be_cleaned])\n",
        "        return tfidf_vectors\n",
        "\n",
        "    # main call function\n",
        "    def doall(self):\n",
        "        cleaned_sentences, cleaned_sentences_with_stopwords = self.cleanall()\n",
        "        tfidf = self.TFIDF(cleaned_sentences)\n",
        "        return [cleaned_sentences,cleaned_sentences_with_stopwords,tfidf]\n",
        "  \n",
        "class TS_SS:\n",
        "    \n",
        "    #cosine similarity\n",
        "    def Cosine(self, question_vector, sentence_vector):\n",
        "        dot_product = np.dot(question_vector, sentence_vector.T)\n",
        "        denominator = (np.linalg.norm(question_vector) * np.linalg.norm(sentence_vector))\n",
        "        return dot_product/denominator\n",
        "    \n",
        "    #Euclidean distance\n",
        "    def Euclidean(self, question_vector, sentence_vector):\n",
        "        vec1 = question_vector.copy()\n",
        "        vec2 = sentence_vector.copy()\n",
        "        if len(vec1)<len(vec2): vec1,vec2 = vec2,vec1\n",
        "        vec2 = np.resize(vec2,(vec1.shape[0],vec1.shape[1]))\n",
        "        return np.linalg.norm(vec1-vec2)\n",
        "    \n",
        "    # angle between two vectors\n",
        "    def Theta(self, question_vector, sentence_vector):\n",
        "        return np.arccos(self.Cosine(question_vector, sentence_vector)) + np.radians(10)\n",
        "    \n",
        "    # triangle formed by two vectors and ED as third side\n",
        "    def Triangle(self, question_vector, sentence_vector):\n",
        "        theta = np.radians(self.Theta(question_vector, sentence_vector))\n",
        "        return ((np.linalg.norm(question_vector) * np.linalg.norm(sentence_vector)) * np.sin(theta))/2\n",
        "    \n",
        "    # difference in magnitude of two vectors\n",
        "    def Magnitude_Difference(self, vec1, vec2):\n",
        "        return abs((np.linalg.norm(vec1) - np.linalg.norm(vec2)))\n",
        "    \n",
        "    # sector area similarity\n",
        "    def Sector(self, question_vector, sentence_vector):\n",
        "        ED = self.Euclidean(question_vector, sentence_vector)\n",
        "        MD = self.Magnitude_Difference(question_vector, sentence_vector)\n",
        "        theta = self.Theta(question_vector, sentence_vector)\n",
        "        return math.pi * (ED + MD)**2 * theta/360\n",
        "\n",
        "    #function which is acivated on call\n",
        "    def __call__(self, question_vector, sentence_vector,method):\n",
        "        if method==1: return self.Euclidean(question_vector, sentence_vector)\n",
        "        elif method==2: return self.Cosine(question_vector, sentence_vector)\n",
        "        else: return self.Triangle(question_vector, sentence_vector) * self.Sector(question_vector, sentence_vector)\n",
        "        \n",
        "        \n",
        "def RetrieveAnswer(question_embedding, tfidf_vectors,method=1):\n",
        "    similarity_heap = []\n",
        "    if method==1: max_similarity = float('inf')\n",
        "    else: max_similarity = -1\n",
        "    index_similarity = -1\n",
        "\n",
        "    for index, embedding in enumerate(tfidf_vectors):  \n",
        "        find_similarity = TS_SS()\n",
        "        similarity = find_similarity((question_embedding).toarray(),(embedding).toarray(),method).mean()\n",
        "        if method==1:\n",
        "            heapq.heappush(similarity_heap,(similarity,index))\n",
        "        elif method==2:\n",
        "            heapq.heappush(similarity_heap,(-similarity,index))\n",
        "        else:\n",
        "            heapq.heappush(similarity_heap,(similarity,index))\n",
        "    return similarity_heap\n",
        "  \n",
        "# question here\n",
        "user_question = \"How to increase mortgage amount ? Do I need more loan sections?\"\n",
        "#define method\n",
        "method = 3\n",
        "\n",
        "preprocess = Preprocessing(txt)\n",
        "cleaned_sentences,cleaned_sentences_with_stopwords,tfidf_vectors = preprocess.doall()\n",
        "question = preprocess.clean_sentence(user_question, stopwords=True)\n",
        "question_embedding = preprocess.TFIDF_Q(question)\n",
        "similarity_heap = RetrieveAnswer(question_embedding , tfidf_vectors ,method)\n",
        "\n",
        "print(\"Question: \", user_question)\n",
        "\n",
        "# number of relevant solutions you want\n",
        "number_of_sentences_to_print = 5\n",
        "while number_of_sentences_to_print>0 and len(similarity_heap)>0:\n",
        "    x = similarity_heap.pop(0)\n",
        "    print(cleaned_sentences_with_stopwords[x[1]])\n",
        "    number_of_sentences_to_print-=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S-AzO-dfAXXL"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}